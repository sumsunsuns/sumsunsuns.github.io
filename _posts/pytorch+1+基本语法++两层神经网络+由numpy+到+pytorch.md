

```python
import numpy

```


```python
import torch
```

引入pytorch包



```python
x=torch.empty(5,3)
x
```




    tensor([[0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.]])




```python
torch.rand(5,3)
```




    tensor([[0.5912, 0.5129, 0.4113],
            [0.3265, 0.0428, 0.9654],
            [0.2721, 0.8113, 0.6526],
            [0.8082, 0.9958, 0.9947],
            [0.4678, 0.4289, 0.2598]])




```python
torch.zeros(5,3)
```




    tensor([[0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.]])




```python
x=torch.zeros(5,3).long()
x.dtype
```




    torch.int64




```python
x=torch.tensor([5.5,3])
x
```




    tensor([5.5000, 3.0000])




```python
x=x.new_ones(5,3,dtype=torch.double)
x
```




    tensor([[1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.]], dtype=torch.float64)




```python
x=torch.rand_like(x,dtype=torch.float)
x
```




    tensor([[0.6762, 0.5497, 0.2873],
            [0.0372, 0.0382, 0.8996],
            [0.7250, 0.6460, 0.5992],
            [0.1896, 0.5026, 0.1651],
            [0.6537, 0.7366, 0.1700]])




```python
x.shape
```




    torch.Size([5, 3])




```python
y=torch.rand(5,3)
```


```python
y
```




    tensor([[0.0966, 0.1602, 0.2338],
            [0.8134, 0.7093, 0.8049],
            [0.8243, 0.5189, 0.6577],
            [0.6979, 0.1550, 0.6573],
            [0.9342, 0.5633, 0.9044]])




```python
x

```




    tensor([[0.6762, 0.5497, 0.2873],
            [0.0372, 0.0382, 0.8996],
            [0.7250, 0.6460, 0.5992],
            [0.1896, 0.5026, 0.1651],
            [0.6537, 0.7366, 0.1700]])




```python
x+y
```




    tensor([[0.7728, 0.7098, 0.5211],
            [0.8505, 0.7475, 1.7045],
            [1.5493, 1.1649, 1.2570],
            [0.8875, 0.6576, 0.8224],
            [1.5879, 1.2999, 1.0744]])




```python
torch.add(x,y)
```




    tensor([[0.7728, 0.7098, 0.5211],
            [0.8505, 0.7475, 1.7045],
            [1.5493, 1.1649, 1.2570],
            [0.8875, 0.6576, 0.8224],
            [1.5879, 1.2999, 1.0744]])




```python
result = torch.empty(5,3)
```


```python
torch.add(x,y,out=result)
```




    tensor([[0.7728, 0.7098, 0.5211],
            [0.8505, 0.7475, 1.7045],
            [1.5493, 1.1649, 1.2570],
            [0.8875, 0.6576, 0.8224],
            [1.5879, 1.2999, 1.0744]])




```python
result
```




    tensor([[0.7728, 0.7098, 0.5211],
            [0.8505, 0.7475, 1.7045],
            [1.5493, 1.1649, 1.2570],
            [0.8875, 0.6576, 0.8224],
            [1.5879, 1.2999, 1.0744]])




```python
y.add_(x)       
```




    tensor([[0.7728, 0.7098, 0.5211],
            [0.8505, 0.7475, 1.7045],
            [1.5493, 1.1649, 1.2570],
            [0.8875, 0.6576, 0.8224],
            [1.5879, 1.2999, 1.0744]])



# 类似于 += 


```python
x=torch.randn(4,4)
```


```python
x
```




    tensor([[-0.1789,  1.1109, -0.6629, -0.0532],
            [-2.3352, -0.3820, -0.0404,  1.0286],
            [ 1.6406, -0.5969, -0.1250, -0.0386],
            [-0.3165,  1.4379,  0.5350, -0.2868]])




```python
y=x.view(2,8)
y
```




    tensor([[-0.1789,  1.1109, -0.6629, -0.0532, -2.3352, -0.3820, -0.0404,  1.0286],
            [ 1.6406, -0.5969, -0.1250, -0.0386, -0.3165,  1.4379,  0.5350, -0.2868]])




```python
y=x.view(-1,8)
y 
```




    tensor([[-0.1789,  1.1109, -0.6629, -0.0532, -2.3352, -0.3820, -0.0404,  1.0286],
            [ 1.6406, -0.5969, -0.1250, -0.0386, -0.3165,  1.4379,  0.5350, -0.2868]])



## -1自动算出 为2


```python
x.grad
```


```python
x

```




    tensor([[-0.1789,  1.1109, -0.6629, -0.0532],
            [-2.3352, -0.3820, -0.0404,  1.0286],
            [ 1.6406, -0.5969, -0.1250, -0.0386],
            [-0.3165,  1.4379,  0.5350, -0.2868]])




```python
x.item()
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-26-3396a1b2b617> in <module>
    ----> 1 x.item()
    

    ValueError: only one element tensors can be converted to Python scalars



```python
x=torch.randn(1)
x
```


```python
x.item()
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-27-3396a1b2b617> in <module>
    ----> 1 x.item()
    

    ValueError: only one element tensors can be converted to Python scalars



```python

```

### 转置矩阵


```python
y.transpose(1,0)
```




    tensor([[-0.1789,  1.6406],
            [ 1.1109, -0.5969],
            [-0.6629, -0.1250],
            [-0.0532, -0.0386],
            [-2.3352, -0.3165],
            [-0.3820,  1.4379],
            [-0.0404,  0.5350],
            [ 1.0286, -0.2868]])




```python

```

pytorch

### numpy 两层神经网络

- $h=W_1X$
- $a=max（0，h）$
- $y_{hat}=W_2a$

计算过程
- forward pass
- loss
- backward pass



```python
import numpy as np
```





```python
import numpy as np
N,D_in,H,D_out=64,1000,100,10

# 随机创建一些训练数据

x=np.random.randn(N,D_in)
y=np.random.randn(N,D_out)

w1= np.random.randn(D_in,H)
w2= np.random.randn(H,D_out)

learn_rate= 1e-6
for t in range(500):
    # forward  pass
    h=x.dot(w1)
    h_relu=np.maximum(h,0)  # N*H   矩阵 行列
    y_pred=h_relu.dot(w2)   # N*D_out
    
    # loss
    loss=np.square(y_pred-y).sum()
    
    print(t,loss)
    
    # backward pass
    # compute the gradient
    grad_y_pred = 2*(y_pred-y)
    
    grad_w2=h_relu.T.dot(grad_y_pred)
    
    
    grad_h_relu= grad_y_pred.dot(w2.T)
    
    grad_h = grad_h_relu.copy()
    grad_h[h<0] = 0
    grad_w1 = x.T.dot(grad_h)
    
    
    
    #update weights of w1 and w2
    
    w1-= learn_rate*grad_w1
    w2-= learn_rate*grad_w2
    
    
    
```

    0 40638227.81618358
    1 40739820.15265566
    2 42080444.04724151
    3 36548933.54396868
    4 24323693.90866805
    5 12676420.3109966
    6 5948660.273190645
    7 3028617.300016407
    8 1841526.457155066
    9 1310580.7023003348
    10 1024400.1969272236
    11 839166.8689926976
    12 703827.04283161
    13 598198.2065237957
    14 512863.8356881297
    15 442799.472274878
    16 384467.2062459412
    17 335540.9602089973
    18 294195.2952239313
    19 258984.80106208622
    20 228912.16413867014
    21 203052.57561991655
    22 180680.22462623112
    23 161241.43685749325
    24 144300.63302232188
    25 129485.92362054756
    26 116475.24537866385
    27 104996.21282697246
    28 94848.18394163059
    29 85842.28591396105
    30 77823.32278739866
    31 70664.6542285606
    32 64273.78031531709
    33 58549.97959600865
    34 53420.2215481556
    35 48804.34622201381
    36 44644.07181162192
    37 40889.367671486616
    38 37495.217626187936
    39 34421.1497066345
    40 31629.165284107192
    41 29092.858438539923
    42 26785.55069199045
    43 24681.424336888802
    44 22761.407897816945
    45 21007.67440055032
    46 19404.96876929481
    47 17938.78877346961
    48 16594.686732987146
    49 15361.487933470526
    50 14229.039176190305
    51 13185.006970472074
    52 12225.047325561747
    53 11341.684645722975
    54 10527.478067994643
    55 9776.37582981919
    56 9083.589732503939
    57 8443.94886101875
    58 7852.673060470016
    59 7306.328158133221
    60 6800.657271373674
    61 6332.480601564725
    62 5898.876659520557
    63 5497.17961198869
    64 5124.5239853850835
    65 4778.849613245845
    66 4457.998234104918
    67 4160.00164958361
    68 3883.2387270941585
    69 3627.4473113510185
    70 3389.4270112951876
    71 3168.044544438467
    72 2962.043483633064
    73 2770.0873629766766
    74 2591.2711039864207
    75 2424.622955960846
    76 2269.3200683873197
    77 2124.4465215932214
    78 1989.2715536873156
    79 1863.155756410739
    80 1745.4545959045909
    81 1635.5529564901544
    82 1532.8903816890122
    83 1436.944899484935
    84 1347.2397701735704
    85 1263.392655423485
    86 1185.0107604787468
    87 1111.7229826634507
    88 1043.1223730198903
    89 978.916178713499
    90 918.8471586321623
    91 862.5788551677673
    92 809.884198005898
    93 760.5431941862364
    94 714.3292622188924
    95 670.9989820176858
    96 630.4044635555525
    97 592.3514784220029
    98 556.6654696129813
    99 523.1984071996201
    100 491.82421339376555
    101 462.38950402970136
    102 434.7619482502545
    103 408.84826828209003
    104 384.5116559368187
    105 361.67746826739847
    106 340.2455290409274
    107 320.11146314494067
    108 301.2022597897905
    109 283.44427607345006
    110 266.76382351629786
    111 251.0898544499151
    112 236.36151644070162
    113 222.52396251842947
    114 209.50359243615253
    115 197.27012246542395
    116 185.76633600842104
    117 174.9542265153979
    118 164.7810729410516
    119 155.2184602253507
    120 146.2226284048309
    121 137.76298552700374
    122 129.80302077036293
    123 122.31391251299607
    124 115.26858901585466
    125 108.63792272763266
    126 102.39840548482604
    127 96.52327185618127
    128 90.99157049389584
    129 85.78544485696617
    130 80.8833549867858
    131 76.26778524115238
    132 71.92106521029437
    133 67.82621523036897
    134 63.9719631242631
    135 60.33976764675059
    136 56.91714854960617
    137 53.69247087379378
    138 50.65585275861061
    139 47.79350359332179
    140 45.095573706715
    141 42.552342798410514
    142 40.15574644529705
    143 37.89853364783927
    144 35.76918681321131
    145 33.76143165842487
    146 31.868350909487372
    147 30.0842245924544
    148 28.401094634647688
    149 26.813439470907003
    150 25.316734872604606
    151 23.90588182678848
    152 22.57501429817466
    153 21.319036040730552
    154 20.134225819107822
    155 19.016130936813916
    156 17.961676667028293
    157 16.96673389659821
    158 16.027478675068227
    159 15.141073770139231
    160 14.304620872640058
    161 13.515169708808937
    162 12.770321534353513
    163 12.066706269171757
    164 11.402871088399984
    165 10.775799214574104
    166 10.183921739546268
    167 9.625133921587725
    168 9.09728759216064
    169 8.599021854035097
    170 8.128265994089945
    171 7.683930692872806
    172 7.264256094086136
    173 6.867743710412793
    174 6.493117786056105
    175 6.139235652978639
    176 5.804898333280931
    177 5.489268622845464
    178 5.191029130753523
    179 4.909073321912311
    180 4.642736433415388
    181 4.390985442880188
    182 4.1532452415088725
    183 3.928495100361411
    184 3.7159840135627347
    185 3.5151705689945714
    186 3.3253590459577698
    187 3.1460353445818816
    188 2.9764614505423705
    189 2.8161648681921543
    190 2.6645572528651686
    191 2.521291340039447
    192 2.385803298536068
    193 2.2577376301092813
    194 2.1365769578032183
    195 2.022035274659988
    196 1.9137161974517563
    197 1.811246042345021
    198 1.7144011574364086
    199 1.6227960113770215
    200 1.536156557748725
    201 1.4541672781862345
    202 1.3765924500479394
    203 1.3032155992880685
    204 1.233844452355538
    205 1.1682101196296881
    206 1.1060887585920602
    207 1.0473237944323992
    208 0.9917032271393637
    209 0.939077863949475
    210 0.8893075815207108
    211 0.8421825468192776
    212 0.7975835386881187
    213 0.7553775456945818
    214 0.7154444134413689
    215 0.6776514128105746
    216 0.6418899590456445
    217 0.6080135436494016
    218 0.5759482921223668
    219 0.545600134335225
    220 0.5168654973473111
    221 0.48967713857666
    222 0.46393600320318396
    223 0.4395530219627088
    224 0.4164776637760646
    225 0.3946149980267746
    226 0.37391507372533067
    227 0.3543174754020062
    228 0.33577242374647837
    229 0.3181944403165953
    230 0.30154474888842947
    231 0.2857795324261907
    232 0.2708497277140087
    233 0.2567123035177862
    234 0.24332321651402786
    235 0.2306365187670918
    236 0.2186136824034352
    237 0.2072260915421597
    238 0.19644206756403462
    239 0.18622293910080023
    240 0.17654655719368412
    241 0.1673763792631465
    242 0.15868819485691804
    243 0.1504548924214003
    244 0.1426517826076135
    245 0.13525827923430728
    246 0.12825414319883538
    247 0.12161489853722929
    248 0.11532425775884605
    249 0.1093641135591467
    250 0.10371220021345173
    251 0.09835623716464582
    252 0.0932808104496962
    253 0.08847130752371857
    254 0.08390953576929376
    255 0.07958555158321787
    256 0.07548775882088934
    257 0.0716047618811793
    258 0.06792201586104701
    259 0.06443151194500955
    260 0.0611204638748138
    261 0.0579818039918864
    262 0.055006420978383305
    263 0.05218635734470704
    264 0.04951086120328098
    265 0.04697559552577242
    266 0.044570288667627606
    267 0.04228888109589819
    268 0.040126217981581815
    269 0.03807560369917546
    270 0.03612988524325594
    271 0.03428477256594588
    272 0.03253578469869366
    273 0.030876637777820057
    274 0.029302119279390713
    275 0.027809041463756772
    276 0.026392782838524254
    277 0.025048980651483276
    278 0.023775000219564503
    279 0.022566140624328546
    280 0.02141919123526351
    281 0.02033139714399711
    282 0.01929921677430212
    283 0.01832009695576859
    284 0.017390688105043968
    285 0.016509286962099953
    286 0.015672700001151976
    287 0.01487883565883397
    288 0.014126097667571197
    289 0.013411543342217606
    290 0.012733202559534479
    291 0.012089550540163769
    292 0.011479005032106237
    293 0.010899383535835312
    294 0.010349236099693165
    295 0.009827377738819854
    296 0.009331901781372038
    297 0.008861534812051132
    298 0.008415256815151272
    299 0.007991730056128805
    300 0.007589506447991277
    301 0.007207688527875598
    302 0.0068453824821294185
    303 0.006501430664323153
    304 0.006174854846779712
    305 0.0058648523172681235
    306 0.005570556013379975
    307 0.005291033760460468
    308 0.005025843796230694
    309 0.004773977437860266
    310 0.004534857594336852
    311 0.004307736619418724
    312 0.004092228759195592
    313 0.0038874877627524985
    314 0.00369317174505369
    315 0.003508561745695066
    316 0.0033332203515489553
    317 0.0031667609118090105
    318 0.0030086746923742337
    319 0.0028586629121485105
    320 0.002716119513954955
    321 0.0025807060942920524
    322 0.0024520828178102665
    323 0.0023298999681007674
    324 0.0022139015919097325
    325 0.0021037588497643555
    326 0.0019991561889378803
    327 0.001899747692607719
    328 0.0018053108322971531
    329 0.0017155886343337564
    330 0.0016303783587095463
    331 0.0015494460391298694
    332 0.0014725806476374567
    333 0.0013995788245463534
    334 0.0013301558256327672
    335 0.0012642178303405398
    336 0.001201572799536648
    337 0.0011420422086442091
    338 0.0010855281407000203
    339 0.001031833528501205
    340 0.0009808025535108554
    341 0.0009323050042551811
    342 0.0008862078021514301
    343 0.0008424043863391855
    344 0.0008007928080967294
    345 0.0007612816153422955
    346 0.0007237099014001111
    347 0.0006880198406697916
    348 0.0006540834113652376
    349 0.00062182623985133
    350 0.0005911756661124107
    351 0.0005620644806414187
    352 0.0005343957451946844
    353 0.0005080847826321755
    354 0.0004830884952727163
    355 0.00045931860791404995
    356 0.0004367303155193539
    357 0.0004152707733293419
    358 0.0003948684504754556
    359 0.0003754705212668718
    360 0.00035703003371808685
    361 0.0003395099520694561
    362 0.0003228487047500422
    363 0.0003070203211353136
    364 0.000291962410235541
    365 0.00027765213159215967
    366 0.00026404515831756026
    367 0.0002511094995846155
    368 0.00023882197088752772
    369 0.0002271337200573812
    370 0.00021601484604566793
    371 0.00020544573435989934
    372 0.00019539989086992808
    373 0.00018584402310886704
    374 0.00017676673965314778
    375 0.000168136143282954
    376 0.00015992187033432977
    377 0.00015211159779512402
    378 0.00014468675946877416
    379 0.00013762715960881795
    380 0.00013091842400565957
    381 0.0001245332122694498
    382 0.00011846454151443573
    383 0.00011268967664741618
    384 0.00010719882313006775
    385 0.00010198038141782537
    386 9.701767158602122e-05
    387 9.229453224634824e-05
    388 8.780237363720779e-05
    389 8.353304296028914e-05
    390 7.946986631110064e-05
    391 7.560870525757276e-05
    392 7.193522840918037e-05
    393 6.843988196877e-05
    394 6.511489696708649e-05
    395 6.19520575801642e-05
    396 5.894612521701556e-05
    397 5.6087929650716766e-05
    398 5.336708779290405e-05
    399 5.07780051079772e-05
    400 4.831564165806222e-05
    401 4.597291193208157e-05
    402 4.374540003274718e-05
    403 4.1628951652464583e-05
    404 3.96129658520203e-05
    405 3.769428350771628e-05
    406 3.586918941844487e-05
    407 3.4133114654196375e-05
    408 3.248239661460856e-05
    409 3.091147256526643e-05
    410 2.941802626344969e-05
    411 2.7995743433318926e-05
    412 2.6642456009057832e-05
    413 2.535513081052162e-05
    414 2.4131136420896406e-05
    415 2.296566839757036e-05
    416 2.1857058552326407e-05
    417 2.0802681547859922e-05
    418 1.9798633963159203e-05
    419 1.8843816283563257e-05
    420 1.7935298542839772e-05
    421 1.7070442153309873e-05
    422 1.624769508030638e-05
    423 1.5464632672818333e-05
    424 1.4719943081274297e-05
    425 1.4011293611221126e-05
    426 1.3336554845411965e-05
    427 1.269432865282619e-05
    428 1.2083310626875634e-05
    429 1.150191913003206e-05
    430 1.0948712566480346e-05
    431 1.0422567929312849e-05
    432 9.921425967844523e-06
    433 9.444360239511764e-06
    434 8.990420838307599e-06
    435 8.558445700948437e-06
    436 8.147549988962578e-06
    437 7.75615089080271e-06
    438 7.384012674016066e-06
    439 7.0294803451987855e-06
    440 6.692076613828205e-06
    441 6.371101896457672e-06
    442 6.065494737189109e-06
    443 5.774495882629925e-06
    444 5.497624918229516e-06
    445 5.234218590385367e-06
    446 4.983376022287627e-06
    447 4.7447308360873275e-06
    448 4.5173986905769966e-06
    449 4.300963578863101e-06
    450 4.09498873421145e-06
    451 3.898886890361025e-06
    452 3.7125543037369235e-06
    453 3.5348834514838032e-06
    454 3.365702971824553e-06
    455 3.204662406247965e-06
    456 3.0513936789320186e-06
    457 2.905511029766739e-06
    458 2.7667102990868542e-06
    459 2.634541697098386e-06
    460 2.508615328476409e-06
    461 2.388733629779633e-06
    462 2.2746237159425244e-06
    463 2.1660674471680484e-06
    464 2.0626537624506892e-06
    465 1.9641687797584347e-06
    466 1.8704707856533965e-06
    467 1.7811963775231441e-06
    468 1.696264786077513e-06
    469 1.6153703888247316e-06
    470 1.5383296873973918e-06
    471 1.4649628053443829e-06
    472 1.3951067530112484e-06
    473 1.3286606346344957e-06
    474 1.2653854854815765e-06
    475 1.2051043980440692e-06
    476 1.147689499255241e-06
    477 1.0930158884695972e-06
    478 1.0409532317276118e-06
    479 9.914276813399544e-07
    480 9.442696116604663e-07
    481 8.993384974527836e-07
    482 8.565357742402845e-07
    483 8.157800496587844e-07
    484 7.769956473706166e-07
    485 7.400411056164031e-07
    486 7.048512998864474e-07
    487 6.71375186168697e-07
    488 6.394543518016947e-07
    489 6.090695386822246e-07
    490 5.801358196955343e-07
    491 5.525708149118207e-07
    492 5.263303973163959e-07
    493 5.013347392708796e-07
    494 4.775482886972366e-07
    495 4.548869740600257e-07
    496 4.3328911057058303e-07
    497 4.1272127570021924e-07
    498 3.9314098672213277e-07
    499 3.744898486996783e-07
    

# pytorch


```python
import torch
N,D_in,H,D_out=64,1000,100,10

# 随机创建一些训练数据

x=torch.randn(N,D_in)
y=torch.randn(N,D_out)

w1= torch.randn(D_in,H)
w2= torch.randn(H,D_out)

learn_rate= 1e-6
for t in range(500):
    # forward  pass
    h=x.mm(w1)
    h_relu=h.clamp(min=0)  # N*H   矩阵 行列
    y_pred=h_relu.mm(w2)   # N*D_out
    
    # loss
    loss=(y_pred-y).pow(2).sum().item()
    
    print(t,loss)
    
    # backward pass
    # compute the gradient
    grad_y_pred = 2*(y_pred-y)
    
    grad_w2=h_relu.t().mm(grad_y_pred)
    
    
    grad_h_relu= grad_y_pred.mm(w2.t())
    
    grad_h = grad_h_relu.clone()
    grad_h[h<0] = 0
    grad_w1 = x.t().mm(grad_h)
    
    
    
    #update weights of w1 and w2
    
    w1-= learn_rate*grad_w1
    w2-= learn_rate*grad_w2
```

    0 25126176.0
    1 19416438.0
    2 17097148.0
    3 15787759.0
    4 14333728.0
    5 12343602.0
    6 9909114.0
    7 7459510.0
    8 5342321.5
    9 3719789.25
    10 2571473.75
    11 1798352.75
    12 1289826.625
    13 956700.5
    14 735384.0625
    15 584573.4375
    16 478408.03125
    17 400923.75
    18 342240.03125
    19 296282.96875
    20 259256.3125
    21 228710.234375
    22 203025.828125
    23 181126.90625
    24 162228.59375
    25 145772.03125
    26 131353.46875
    27 118642.1796875
    28 107392.0625
    29 97401.3515625
    30 88502.546875
    31 80551.015625
    32 73428.3828125
    33 67034.28125
    34 61276.35546875
    35 56081.07421875
    36 51385.4140625
    37 47141.4765625
    38 43291.9140625
    39 39794.33203125
    40 36614.62890625
    41 33718.96484375
    42 31079.203125
    43 28671.62109375
    44 26471.60546875
    45 24460.220703125
    46 22618.189453125
    47 20929.7265625
    48 19380.126953125
    49 17958.494140625
    50 16652.59765625
    51 15450.66796875
    52 14343.779296875
    53 13320.9140625
    54 12377.6845703125
    55 11507.9794921875
    56 10705.306640625
    57 9963.9228515625
    58 9278.3154296875
    59 8643.841796875
    60 8056.453125
    61 7512.19287109375
    62 7007.5810546875
    63 6539.33544921875
    64 6104.93896484375
    65 5701.55029296875
    66 5326.8759765625
    67 4978.57666015625
    68 4654.66748046875
    69 4353.2646484375
    70 4072.6171875
    71 3811.452880859375
    72 3568.11572265625
    73 3341.22509765625
    74 3129.700439453125
    75 2932.46484375
    76 2748.4072265625
    77 2576.602783203125
    78 2416.21044921875
    79 2266.3310546875
    80 2126.2919921875
    81 1995.353759765625
    82 1872.951904296875
    83 1758.5380859375
    84 1651.441650390625
    85 1551.183349609375
    86 1457.3250732421875
    87 1369.4498291015625
    88 1287.1380615234375
    89 1210.0548095703125
    90 1137.785400390625
    91 1070.0595703125
    92 1006.5506591796875
    93 947.002197265625
    94 891.1671142578125
    95 838.782958984375
    96 789.5781860351562
    97 743.3991088867188
    98 700.1488647460938
    99 659.5557861328125
    100 621.441650390625
    101 585.6010131835938
    102 551.916259765625
    103 520.2432250976562
    104 490.4597473144531
    105 462.4559326171875
    106 436.11334228515625
    107 411.3156433105469
    108 387.9808044433594
    109 366.02069091796875
    110 345.3436279296875
    111 325.88409423828125
    112 307.56011962890625
    113 290.2909851074219
    114 274.0227355957031
    115 258.696533203125
    116 244.25526428222656
    117 230.64691162109375
    118 217.8243865966797
    119 205.73471069335938
    120 194.3324737548828
    121 183.58392333984375
    122 173.44285583496094
    123 163.88003540039062
    124 154.864990234375
    125 146.35472106933594
    126 138.32730102539062
    127 130.74691772460938
    128 123.59814453125
    129 116.84619140625
    130 110.47537231445312
    131 104.45809936523438
    132 98.77989196777344
    133 93.41402435302734
    134 88.34738159179688
    135 83.56165313720703
    136 79.04330444335938
    137 74.77210998535156
    138 70.73860931396484
    139 66.92674255371094
    140 63.3244743347168
    141 59.92059326171875
    142 56.70484924316406
    143 53.66285705566406
    144 50.789764404296875
    145 48.07279968261719
    146 45.504051208496094
    147 43.07482147216797
    148 40.77887725830078
    149 38.607078552246094
    150 36.55269241333008
    151 34.610198974609375
    152 32.77262878417969
    153 31.034652709960938
    154 29.389707565307617
    155 27.834070205688477
    156 26.36239242553711
    157 24.96957015991211
    158 23.65187644958496
    159 22.405139923095703
    160 21.22447967529297
    161 20.106733322143555
    162 19.04978370666504
    163 18.049671173095703
    164 17.102306365966797
    165 16.204771041870117
    166 15.355093002319336
    167 14.551414489746094
    168 13.789871215820312
    169 13.069485664367676
    170 12.386402130126953
    171 11.739896774291992
    172 11.127753257751465
    173 10.54798698425293
    174 9.997995376586914
    175 9.477899551391602
    176 8.985357284545898
    177 8.518270492553711
    178 8.076004981994629
    179 7.6567792892456055
    180 7.259850978851318
    181 6.883520603179932
    182 6.527357578277588
    183 6.189223289489746
    184 5.869289875030518
    185 5.566067695617676
    186 5.2784318923950195
    187 5.0058979988098145
    188 4.747983932495117
    189 4.503154754638672
    190 4.271101474761963
    191 4.051204681396484
    192 3.8427720069885254
    193 3.6449007987976074
    194 3.457766056060791
    195 3.279874086380005
    196 3.1114587783813477
    197 2.9518539905548096
    198 2.8006086349487305
    199 2.6569557189941406
    200 2.5209031105041504
    201 2.3919522762298584
    202 2.2694499492645264
    203 2.1533565521240234
    204 2.0433542728424072
    205 1.9390604496002197
    206 1.8398149013519287
    207 1.7458535432815552
    208 1.6569215059280396
    209 1.5725204944610596
    210 1.4924067258834839
    211 1.4163097143173218
    212 1.34409761428833
    213 1.2756794691085815
    214 1.210854411125183
    215 1.149329662322998
    216 1.0908775329589844
    217 1.0354783535003662
    218 0.982885479927063
    219 0.9329708814620972
    220 0.8856622576713562
    221 0.8407278656959534
    222 0.798140287399292
    223 0.7576479911804199
    224 0.7192592024803162
    225 0.6828159689903259
    226 0.6483260989189148
    227 0.6155271530151367
    228 0.5843608975410461
    229 0.5548477172851562
    230 0.5268718004226685
    231 0.5002561807632446
    232 0.47495102882385254
    233 0.4510119557380676
    234 0.42826324701309204
    235 0.4066116213798523
    236 0.38613250851631165
    237 0.3666989207267761
    238 0.34821516275405884
    239 0.33065688610076904
    240 0.3140823245048523
    241 0.2982466518878937
    242 0.28324857354164124
    243 0.26901018619537354
    244 0.25545910000801086
    245 0.24265308678150177
    246 0.23046307265758514
    247 0.21891175210475922
    248 0.20789703726768494
    249 0.19746403396129608
    250 0.1875828355550766
    251 0.17817139625549316
    252 0.16923323273658752
    253 0.16076865792274475
    254 0.15268602967262268
    255 0.1450696885585785
    256 0.13776209950447083
    257 0.13087835907936096
    258 0.124334916472435
    259 0.11813927441835403
    260 0.1122368648648262
    261 0.10662288963794708
    262 0.10129158198833466
    263 0.09622262418270111
    264 0.09143872559070587
    265 0.08688457310199738
    266 0.08254759013652802
    267 0.0784156396985054
    268 0.07452404499053955
    269 0.07081536948680878
    270 0.06728044152259827
    271 0.06392433494329453
    272 0.060749005526304245
    273 0.05772833153605461
    274 0.05485595017671585
    275 0.052122727036476135
    276 0.04954597353935242
    277 0.047074370086193085
    278 0.04473729059100151
    279 0.04251313954591751
    280 0.040397077798843384
    281 0.038398656994104385
    282 0.03649793565273285
    283 0.034682564437389374
    284 0.032962340861558914
    285 0.03134223073720932
    286 0.029782826080918312
    287 0.02831905148923397
    288 0.02691498026251793
    289 0.025591406971216202
    290 0.024332217872142792
    291 0.023127928376197815
    292 0.02199745550751686
    293 0.020911039784550667
    294 0.019887536764144897
    295 0.018909446895122528
    296 0.01797410659492016
    297 0.017087800428271294
    298 0.016258571296930313
    299 0.015463821589946747
    300 0.014701835811138153
    301 0.013983221724629402
    302 0.013303767889738083
    303 0.012658020481467247
    304 0.012032335624098778
    305 0.011452440172433853
    306 0.0109019223600626
    307 0.010374024510383606
    308 0.009877733886241913
    309 0.009395323693752289
    310 0.00894029438495636
    311 0.008510848507285118
    312 0.008107672445476055
    313 0.007718592416495085
    314 0.007350529544055462
    315 0.007000898011028767
    316 0.006670982576906681
    317 0.006351666525006294
    318 0.006052865646779537
    319 0.005771100055426359
    320 0.0054987636394798756
    321 0.0052456120029091835
    322 0.004997617565095425
    323 0.004761329852044582
    324 0.0045384750701487064
    325 0.004326885100454092
    326 0.004129489418119192
    327 0.003939565271139145
    328 0.0037602076772600412
    329 0.0035885144025087357
    330 0.0034292612690478563
    331 0.003274706657975912
    332 0.0031254375353455544
    333 0.002986375242471695
    334 0.0028546256944537163
    335 0.0027308547869324684
    336 0.0026109865866601467
    337 0.002494960557669401
    338 0.002386388136073947
    339 0.0022831226233392954
    340 0.00218660244718194
    341 0.002092418959364295
    342 0.002005870221182704
    343 0.0019212807528674603
    344 0.0018410009797662497
    345 0.0017664592014625669
    346 0.001694517326541245
    347 0.0016249462496489286
    348 0.0015584242064505816
    349 0.001497225952334702
    350 0.0014354810118675232
    351 0.0013798270374536514
    352 0.0013242213753983378
    353 0.0012709605507552624
    354 0.0012217584298923612
    355 0.001174083910882473
    356 0.0011300669284537435
    357 0.0010870901169255376
    358 0.0010447605745866895
    359 0.0010076207108795643
    360 0.0009690705337561667
    361 0.0009327768930234015
    362 0.0008972717332653701
    363 0.0008657046128064394
    364 0.0008345040259882808
    365 0.0008063963032327592
    366 0.0007759262225590646
    367 0.0007486097165383399
    368 0.0007219896069727838
    369 0.0006981092155911028
    370 0.0006730224704369903
    371 0.0006502619362436235
    372 0.000627444009296596
    373 0.0006063065375201404
    374 0.0005875646602362394
    375 0.0005676689324900508
    376 0.0005488148890435696
    377 0.0005322541110217571
    378 0.0005150380893610418
    379 0.0004984555998817086
    380 0.00048283167416229844
    381 0.00046737154480069876
    382 0.00045329489512369037
    383 0.0004389857640489936
    384 0.0004270892823114991
    385 0.0004131013120058924
    386 0.00040079891914501786
    387 0.0003888264182023704
    388 0.0003763727145269513
    389 0.0003670151927508414
    390 0.00035604354343377054
    391 0.00034594169119372964
    392 0.00033508523483760655
    393 0.00032586607267148793
    394 0.000317225611070171
    395 0.00030822481494396925
    396 0.00029970225295983255
    397 0.0002910494222305715
    398 0.00028206195565871894
    399 0.00027646898524835706
    400 0.00026807456742972136
    401 0.00026109765167348087
    402 0.00025439547607675195
    403 0.0002475375367794186
    404 0.00024170943652279675
    405 0.00023462374520022422
    406 0.00022888233070261776
    407 0.00022264974541030824
    408 0.0002176223206333816
    409 0.00021248336997814476
    410 0.00020654971012845635
    411 0.00020168557239230722
    412 0.00019678034004755318
    413 0.00019185834389645606
    414 0.00018739639199338853
    415 0.00018297505448572338
    416 0.00017884973203763366
    417 0.00017425886471755803
    418 0.00017003790708258748
    419 0.00016620868700556457
    420 0.0001625107106519863
    421 0.00015878569683991373
    422 0.0001551382738398388
    423 0.00015175205771811306
    424 0.0001486512046540156
    425 0.00014553169603459537
    426 0.0001422021014150232
    427 0.00013892243441659957
    428 0.00013608942390419543
    429 0.0001327091595157981
    430 0.00013017421588301659
    431 0.00012791395420208573
    432 0.00012568193778861314
    433 0.00012275660992600024
    434 0.0001197636011056602
    435 0.0001175992947537452
    436 0.00011535756493685767
    437 0.00011288801033515483
    438 0.00011088528844993562
    439 0.0001089476645574905
    440 0.00010657048551365733
    441 0.00010491511784493923
    442 0.00010248307808069512
    443 0.00010020504123531282
    444 9.868385677691549e-05
    445 9.667942504165694e-05
    446 9.504954505246133e-05
    447 9.30914466152899e-05
    448 9.143394709099084e-05
    449 9.001650323625654e-05
    450 8.833202446112409e-05
    451 8.676071593072265e-05
    452 8.537880057701841e-05
    453 8.375695324502885e-05
    454 8.224988414440304e-05
    455 8.091158815659583e-05
    456 7.931846630526707e-05
    457 7.846749213058501e-05
    458 7.687181641813368e-05
    459 7.556082709925249e-05
    460 7.3936753324233e-05
    461 7.29122111806646e-05
    462 7.190264295786619e-05
    463 7.121505041141063e-05
    464 7.009283581282943e-05
    465 6.876578845549375e-05
    466 6.742692494299263e-05
    467 6.619554187636822e-05
    468 6.51053196634166e-05
    469 6.405997555702925e-05
    470 6.310555909294635e-05
    471 6.22862862655893e-05
    472 6.124845094745979e-05
    473 6.048423529136926e-05
    474 5.962529394309968e-05
    475 5.8714591432362795e-05
    476 5.7771649153437465e-05
    477 5.739861080655828e-05
    478 5.634254921460524e-05
    479 5.55748883925844e-05
    480 5.482348569785245e-05
    481 5.4098891268949956e-05
    482 5.3331314120441675e-05
    483 5.2581348427338526e-05
    484 5.181880260352045e-05
    485 5.112511644256301e-05
    486 5.024467100156471e-05
    487 4.964844629284926e-05
    488 4.889710544375703e-05
    489 4.816069849766791e-05
    490 4.768395228893496e-05
    491 4.6959412429714575e-05
    492 4.6419849240919575e-05
    493 4.585890201269649e-05
    494 4.504638854996301e-05
    495 4.4515378249343485e-05
    496 4.4080414227209985e-05
    497 4.351605093688704e-05
    498 4.2854357161559165e-05
    499 4.255767271388322e-05
    

### 继续修改


```python
import torch

N,D_in,H,D_out=64,1000,100,10

# 随机创建一些训练数据

x=torch.randn(N,D_in)
y=torch.randn(N,D_out)

w1= torch.randn(D_in,H,requires_grad=True)
w2= torch.randn(H,D_out,requires_grad=True)

learn_rate= 1e-6
for t in range(500):
    
    
    y_pred=x.mm(w1).clamp(min=0).mm(w2)
    # loss
    loss=(y_pred-y).pow(2).sum()
    
    print(t,loss.item())                    #item拿下来，因为只能算一个
    
    # backward pass
    # compute the gradient
    
    
    loss.backward()                          #计算图
    
    
    
    #update weights of w1 and w2
    with torch.no_grad():                    #不让之前的占内存
        w1-= learn_rate* w1.grad
        w2-= learn_rate* w2.grad             
        w1.grad.zero_()                      #要把grad清零，不然影响之后的。
        w2.grad.zero_()
```

    0 28612738.0
    1 25215128.0
    2 28813460.0
    3 35609472.0
    4 40153784.0
    5 36085888.0
    6 24014110.0
    7 11940782.0
    8 5171809.5
    9 2382557.0
    10 1346977.25
    11 928467.6875
    12 722922.875
    13 596814.875
    14 506096.15625
    15 434941.65625
    16 376815.1875
    17 328450.21875
    18 287684.625
    19 253164.078125
    20 223629.625
    21 198240.46875
    22 176338.71875
    23 157308.953125
    24 140719.578125
    25 126208.8515625
    26 113451.484375
    27 102210.0625
    28 92269.4921875
    29 83452.5703125
    30 75609.03125
    31 68615.671875
    32 62364.84375
    33 56760.5546875
    34 51732.49609375
    35 47215.8359375
    36 43149.21875
    37 39480.8671875
    38 36166.03515625
    39 33166.6640625
    40 30446.640625
    41 27976.69921875
    42 25730.50390625
    43 23686.78515625
    44 21824.431640625
    45 20124.978515625
    46 18571.5859375
    47 17150.87890625
    48 15849.9453125
    49 14657.5556640625
    50 13564.2265625
    51 12559.998046875
    52 11637.294921875
    53 10788.806640625
    54 10007.79296875
    55 9288.4599609375
    56 8625.3125
    57 8013.8046875
    58 7449.52880859375
    59 6928.50830078125
    60 6447.0517578125
    61 6001.583984375
    62 5589.345703125
    63 5207.65673828125
    64 4853.9833984375
    65 4526.09765625
    66 4221.55810546875
    67 3939.175048828125
    68 3677.392578125
    69 3434.2353515625
    70 3208.252197265625
    71 2998.210693359375
    72 2802.88671875
    73 2621.208740234375
    74 2452.1015625
    75 2294.66162109375
    76 2148.0048828125
    77 2011.3778076171875
    78 1883.9444580078125
    79 1765.09423828125
    80 1654.22119140625
    81 1550.765380859375
    82 1454.209716796875
    83 1363.9886474609375
    84 1279.7276611328125
    85 1201.003173828125
    86 1127.4779052734375
    87 1058.693603515625
    88 994.3466186523438
    89 934.135498046875
    90 877.7866821289062
    91 825.0277709960938
    92 775.6019897460938
    93 729.3057861328125
    94 685.93017578125
    95 645.273193359375
    96 607.1489868164062
    97 571.4237060546875
    98 537.9429321289062
    99 506.5245361328125
    100 477.0318603515625
    101 449.3465576171875
    102 423.3499450683594
    103 398.9276123046875
    104 375.98699951171875
    105 354.4213562011719
    106 334.1585388183594
    107 315.10174560546875
    108 297.18756103515625
    109 280.3381042480469
    110 264.48529052734375
    111 249.56991577148438
    112 235.53225708007812
    113 222.32525634765625
    114 209.88470458984375
    115 198.16954040527344
    116 187.1368865966797
    117 176.74703979492188
    118 166.95701599121094
    119 157.73031616210938
    120 149.0334930419922
    121 140.83709716796875
    122 133.10902404785156
    123 125.81981658935547
    124 118.9466552734375
    125 112.46560668945312
    126 106.34803009033203
    127 100.57845306396484
    128 95.13262176513672
    129 89.99362182617188
    130 85.14013671875
    131 80.55906677246094
    132 76.23426055908203
    133 72.1491470336914
    134 68.29102325439453
    135 64.64602661132812
    136 61.20265197753906
    137 57.94957733154297
    138 54.87562561035156
    139 51.969505310058594
    140 49.223262786865234
    141 46.62631607055664
    142 44.17316436767578
    143 41.85275650024414
    144 39.65797805786133
    145 37.58301544189453
    146 35.62015914916992
    147 33.76206588745117
    148 32.00485610961914
    149 30.341873168945312
    150 28.76802635192871
    151 27.278427124023438
    152 25.868989944458008
    153 24.533519744873047
    154 23.269746780395508
    155 22.07238006591797
    156 20.939292907714844
    157 19.865089416503906
    158 18.848798751831055
    159 17.88517951965332
    160 16.972827911376953
    161 16.10816192626953
    162 15.288773536682129
    163 14.51270866394043
    164 13.776214599609375
    165 13.078831672668457
    166 12.418388366699219
    167 11.791007041931152
    168 11.19704818725586
    169 10.633774757385254
    170 10.0991849899292
    171 9.592257499694824
    172 9.111781120300293
    173 8.656171798706055
    174 8.223581314086914
    175 7.813296318054199
    176 7.424061298370361
    177 7.054673671722412
    178 6.704185485839844
    179 6.371879577636719
    180 6.055967330932617
    181 5.756941795349121
    182 5.47246789932251
    183 5.202270030975342
    184 4.946024417877197
    185 4.702713966369629
    186 4.471717357635498
    187 4.252203464508057
    188 4.044116020202637
    189 3.846017360687256
    190 3.658165454864502
    191 3.4796688556671143
    192 3.3100831508636475
    193 3.14855694770813
    194 2.995708703994751
    195 2.8503313064575195
    196 2.711836338043213
    197 2.5805912017822266
    198 2.455676555633545
    199 2.3370561599731445
    200 2.22428822517395
    201 2.1173276901245117
    202 2.0154013633728027
    203 1.9185218811035156
    204 1.8263297080993652
    205 1.73887300491333
    206 1.6555936336517334
    207 1.5764330625534058
    208 1.5010972023010254
    209 1.4294486045837402
    210 1.361406683921814
    211 1.2965130805969238
    212 1.2349355220794678
    213 1.1763335466384888
    214 1.1205947399139404
    215 1.0675487518310547
    216 1.0169109106063843
    217 0.9689536094665527
    218 0.923200249671936
    219 0.8797113299369812
    220 0.8382641673088074
    221 0.79877108335495
    222 0.7612268328666687
    223 0.725572943687439
    224 0.69170743227005
    225 0.6592481732368469
    226 0.6283913254737854
    227 0.5990544557571411
    228 0.5710245966911316
    229 0.5444770455360413
    230 0.5191407203674316
    231 0.49501463770866394
    232 0.4719981253147125
    233 0.450117826461792
    234 0.4292193055152893
    235 0.40945887565612793
    236 0.3904629349708557
    237 0.3724918067455292
    238 0.3553374707698822
    239 0.33901914954185486
    240 0.3233528137207031
    241 0.3084673583507538
    242 0.2942892014980316
    243 0.28076937794685364
    244 0.26796358823776245
    245 0.2557081878185272
    246 0.24398362636566162
    247 0.23286283016204834
    248 0.22223660349845886
    249 0.21211613714694977
    250 0.20243924856185913
    251 0.19324147701263428
    252 0.18444837629795074
    253 0.17610900104045868
    254 0.1681467890739441
    255 0.16055947542190552
    256 0.1532936543226242
    257 0.1463845670223236
    258 0.13977381587028503
    259 0.13345028460025787
    260 0.12747016549110413
    261 0.1217157170176506
    262 0.11627543717622757
    263 0.11103585362434387
    264 0.10607961565256119
    265 0.10132406651973724
    266 0.09679485857486725
    267 0.09246360510587692
    268 0.08835288882255554
    269 0.08440825343132019
    270 0.08066496253013611
    271 0.0770491361618042
    272 0.07362524420022964
    273 0.07036038488149643
    274 0.06722819060087204
    275 0.06426191329956055
    276 0.061437346041202545
    277 0.058713532984256744
    278 0.05612972378730774
    279 0.05365690961480141
    280 0.051312875002622604
    281 0.04905141890048981
    282 0.04690710827708244
    283 0.04484717175364494
    284 0.04287330433726311
    285 0.0409989207983017
    286 0.039217591285705566
    287 0.03749620169401169
    288 0.0358702689409256
    289 0.03431049734354019
    290 0.03282642364501953
    291 0.03140553832054138
    292 0.030027370899915695
    293 0.028739847242832184
    294 0.027501368895173073
    295 0.026306550949811935
    296 0.025178302079439163
    297 0.024097882211208344
    298 0.023055236786603928
    299 0.022081082686781883
    300 0.021135903894901276
    301 0.020230157300829887
    302 0.019356902688741684
    303 0.018523380160331726
    304 0.017738329246640205
    305 0.016989683732390404
    306 0.01626652292907238
    307 0.01558853592723608
    308 0.014918419532477856
    309 0.014290541410446167
    310 0.013689802959561348
    311 0.013112258166074753
    312 0.012564889155328274
    313 0.012045192532241344
    314 0.011543729342520237
    315 0.011067153885960579
    316 0.010607619769871235
    317 0.010162277147173882
    318 0.009751331992447376
    319 0.009349439293146133
    320 0.008967461995780468
    321 0.008602227084338665
    322 0.00824736524373293
    323 0.007912909612059593
    324 0.007590563967823982
    325 0.0072882212698459625
    326 0.006991894915699959
    327 0.006718277931213379
    328 0.006449552718549967
    329 0.006196700967848301
    330 0.005949817597866058
    331 0.00571040902286768
    332 0.0054855430498719215
    333 0.005270618945360184
    334 0.005064461845904589
    335 0.004867378156632185
    336 0.0046750507317483425
    337 0.0044938502833247185
    338 0.004318558145314455
    339 0.004148254171013832
    340 0.003991519100964069
    341 0.00383948115631938
    342 0.0036893475335091352
    343 0.003554754890501499
    344 0.003422384848818183
    345 0.0032927875872701406
    346 0.0031704185530543327
    347 0.0030533471144735813
    348 0.0029393250588327646
    349 0.002831022720783949
    350 0.0027298186905682087
    351 0.0026304293423891068
    352 0.002533935010433197
    353 0.0024445634335279465
    354 0.0023567460011690855
    355 0.0022724131122231483
    356 0.0021892650984227657
    357 0.0021164242643862963
    358 0.002041403204202652
    359 0.001973517471924424
    360 0.0019040830666199327
    361 0.0018371036276221275
    362 0.0017756312154233456
    363 0.0017145515885204077
    364 0.0016550810541957617
    365 0.0015996858710423112
    366 0.001548611093312502
    367 0.001497113611549139
    368 0.0014467936707660556
    369 0.0014005252160131931
    370 0.0013544081011787057
    371 0.0013099649222567677
    372 0.0012680157087743282
    373 0.0012295580236241221
    374 0.0011898784432560205
    375 0.0011516180820763111
    376 0.0011160292197018862
    377 0.0010804759804159403
    378 0.0010483904043212533
    379 0.0010167579166591167
    380 0.000984550453722477
    381 0.0009542091866023839
    382 0.0009269851725548506
    383 0.0008989977650344372
    384 0.0008732939604669809
    385 0.0008482367265969515
    386 0.0008228824008256197
    387 0.0007990151643753052
    388 0.0007759998552501202
    389 0.0007549795554950833
    390 0.0007323016179725528
    391 0.0007119718356989324
    392 0.0006931195966899395
    393 0.0006731685716658831
    394 0.000653809227515012
    395 0.0006367251626215875
    396 0.0006184972589835525
    397 0.000601363368332386
    398 0.0005856413044966757
    399 0.0005702926428057253
    400 0.0005551996291615069
    401 0.0005405446281656623
    402 0.0005264142528176308
    403 0.0005127193289808929
    404 0.0004991292371414602
    405 0.0004871203564107418
    406 0.0004742654855363071
    407 0.0004624290158972144
    408 0.00045105471508577466
    409 0.0004391908005345613
    410 0.00042856449726969004
    411 0.0004176518996246159
    412 0.00040774105582386255
    413 0.00039746728725731373
    414 0.0003888081118930131
    415 0.00037932314444333315
    416 0.00037129531847313046
    417 0.0003613764711190015
    418 0.0003532078117132187
    419 0.00034406338818371296
    420 0.0003367319004610181
    421 0.00032971680047921836
    422 0.00032167526660487056
    423 0.00031342761940322816
    424 0.00030651353881694376
    425 0.0003002553421538323
    426 0.0002932364004664123
    427 0.0002869176969397813
    428 0.00028114355518482625
    429 0.00027468582266010344
    430 0.0002690104302018881
    431 0.0002633444673847407
    432 0.00025728397304192185
    433 0.0002520692360121757
    434 0.0002472183550707996
    435 0.00024204565852414817
    436 0.00023646882618777454
    437 0.0002315769379492849
    438 0.00022686287411488593
    439 0.00022218978847377002
    440 0.00021752814063802361
    441 0.00021259661298245192
    442 0.0002081872953567654
    443 0.00020474886696320027
    444 0.00020092701015528291
    445 0.00019650373724289238
    446 0.00019249125034548342
    447 0.00018864430603571236
    448 0.00018530197849031538
    449 0.00018173331045545638
    450 0.00017862387176137418
    451 0.0001750273077050224
    452 0.0001718310231808573
    453 0.0001685301831457764
    454 0.0001656332751736045
    455 0.00016265237354673445
    456 0.0001602336560608819
    457 0.0001568281149957329
    458 0.0001540197990834713
    459 0.00015074657858349383
    460 0.00014846507110632956
    461 0.00014583907613996416
    462 0.00014334701700136065
    463 0.0001409200340276584
    464 0.00013846540241502225
    465 0.00013602593389805406
    466 0.00013336981646716595
    467 0.00013114455214235932
    468 0.00012898189015686512
    469 0.00012674686149694026
    470 0.0001245511812157929
    471 0.00012301884999033064
    472 0.00012071007222402841
    473 0.00011874415940837935
    474 0.00011653010733425617
    475 0.00011423552496125922
    476 0.00011258165613980964
    477 0.00011114402877865359
    478 0.00010930864664260298
    479 0.00010767441563075408
    480 0.0001057597910403274
    481 0.00010384789493400604
    482 0.00010220552212558687
    483 0.00010048810509033501
    484 9.919260628521442e-05
    485 9.784762369235978e-05
    486 9.638542542234063e-05
    487 9.499052248429507e-05
    488 9.326881263405085e-05
    489 9.18668374652043e-05
    490 9.044393664225936e-05
    491 8.89957882463932e-05
    492 8.755557064432651e-05
    493 8.605659240856767e-05
    494 8.480777614749968e-05
    495 8.340638305526227e-05
    496 8.221167081501335e-05
    497 8.138442353811115e-05
    498 8.031633478822187e-05
    499 7.938733324408531e-05
    

## Torch nn库


```python
import torch
import torch.nn as nn

N,D_in,H,D_out=64,1000,100,10

# 随机创建一些训练数据

x=torch.randn(N,D_in)
y=torch.randn(N,D_out)

model = torch.nn.Sequential(
   torch.nn.Linear(D_in,H,bias=False),
   torch.nn.ReLU(),
   torch.nn.Linear(H,D_out,bias=False),

)

torch.nn.init.normal_(model[0].weight)   #第一个liner
torch.nn.init.normal_(model[2].weight)   #第二个liner  因为moder[1]是relu
loss_fn =nn.MSELoss(reduction='sum')

learn_rate= 1e-6
for t in range(500):
    
    
    y_pred =model(x)
    # loss
    loss=loss_fn(y_pred,y)
    
    print(t,loss.item())                    #item拿下来，因为只能算一个
    
    model.zero_grad()
    # backward pass
    # compute the gradient
    
    
    loss.backward()                          #计算图
    
    
    
    #update weights of w1 and w2
    with torch.no_grad():                    #不让之前的占内存
        for param in model.parameters():
              param-=learn_rate*param.grad
                
      
```

    0 29453280.0
    1 27422128.0
    2 27460224.0
    3 25849472.0
    4 21417140.0
    5 15041579.0
    6 9317062.0
    7 5378704.5
    8 3147083.5
    9 1963057.75
    10 1338640.25
    11 989332.4375
    12 776771.6875
    13 634725.25
    14 532000.0
    15 453325.8125
    16 390538.375
    17 339123.75
    18 296269.0625
    19 260213.265625
    20 229618.1875
    21 203459.234375
    22 181038.5625
    23 161732.171875
    24 144990.328125
    25 130402.015625
    26 117642.25
    27 106440.515625
    28 96575.640625
    29 87861.53125
    30 80140.3984375
    31 73276.0234375
    32 67158.53125
    33 61695.296875
    34 56794.01953125
    35 52388.3984375
    36 48416.921875
    37 44829.3984375
    38 41580.02734375
    39 38631.015625
    40 35948.34765625
    41 33504.93359375
    42 31273.16796875
    43 29231.046875
    44 27357.681640625
    45 25635.93359375
    46 24052.279296875
    47 22593.080078125
    48 21244.57421875
    49 19997.5390625
    50 18842.587890625
    51 17770.6953125
    52 16775.123046875
    53 15849.1376953125
    54 14986.275390625
    55 14181.419921875
    56 13429.7265625
    57 12727.28515625
    58 12070.0234375
    59 11453.88671875
    60 10875.9169921875
    61 10333.2080078125
    62 9823.224609375
    63 9343.712890625
    64 8892.0859375
    65 8466.482421875
    66 8065.53759765625
    67 7687.611328125
    68 7330.54345703125
    69 6993.275390625
    70 6674.3505859375
    71 6372.455078125
    72 6086.63623046875
    73 5815.6982421875
    74 5558.7685546875
    75 5315.09619140625
    76 5083.857421875
    77 4864.27685546875
    78 4655.4970703125
    79 4456.98876953125
    80 4268.2626953125
    81 4088.70703125
    82 3917.6552734375
    83 3754.7421875
    84 3599.56591796875
    85 3451.7734375
    86 3310.7529296875
    87 3176.21826171875
    88 3047.845947265625
    89 2925.4052734375
    90 2808.472900390625
    91 2696.75146484375
    92 2590.017822265625
    93 2487.97900390625
    94 2390.509765625
    95 2297.209716796875
    96 2207.967529296875
    97 2122.58642578125
    98 2040.9521484375
    99 1962.75537109375
    100 1887.847412109375
    101 1816.1009521484375
    102 1747.3929443359375
    103 1681.5792236328125
    104 1618.464111328125
    105 1557.9482421875
    106 1499.923583984375
    107 1444.32275390625
    108 1390.938720703125
    109 1339.7315673828125
    110 1290.579345703125
    111 1243.432373046875
    112 1198.149169921875
    113 1154.6639404296875
    114 1112.89306640625
    115 1072.784423828125
    116 1034.2750244140625
    117 997.2385864257812
    118 961.638671875
    119 927.4169921875
    120 894.5610961914062
    121 862.9423828125
    122 832.5296630859375
    123 803.275146484375
    124 775.1492309570312
    125 748.0919189453125
    126 722.0427856445312
    127 697.02978515625
    128 672.9556884765625
    129 649.8095703125
    130 627.4885864257812
    131 606.0001220703125
    132 585.3057861328125
    133 565.3956298828125
    134 546.1998901367188
    135 527.6979370117188
    136 509.8721923828125
    137 492.70037841796875
    138 476.15618896484375
    139 460.1963195800781
    140 444.80706787109375
    141 429.97039794921875
    142 415.68133544921875
    143 401.88922119140625
    144 388.5800476074219
    145 375.7427062988281
    146 363.36676025390625
    147 351.4318542480469
    148 339.9030456542969
    149 328.7763366699219
    150 318.037353515625
    151 307.6901550292969
    152 297.68414306640625
    153 288.0282287597656
    154 278.705078125
    155 269.7127685546875
    156 261.0553894042969
    157 252.68997192382812
    158 244.60870361328125
    159 236.80618286132812
    160 229.27781677246094
    161 221.9917755126953
    162 214.95172119140625
    163 208.1507568359375
    164 201.5834197998047
    165 195.23431396484375
    166 189.09388732910156
    167 183.1564483642578
    168 177.4208526611328
    169 171.87979125976562
    170 166.51519775390625
    171 161.32891845703125
    172 156.31324768066406
    173 151.46640014648438
    174 146.77651977539062
    175 142.2364501953125
    176 137.84414672851562
    177 133.59646606445312
    178 129.49310302734375
    179 125.51612854003906
    180 121.66650390625
    181 117.94188690185547
    182 114.33965301513672
    183 110.8538589477539
    184 107.47698974609375
    185 104.2074966430664
    186 101.0422134399414
    187 97.98199462890625
    188 95.01905822753906
    189 92.14591217041016
    190 89.36369323730469
    191 86.67120361328125
    192 84.0656967163086
    193 81.53897857666016
    194 79.09135437011719
    195 76.72245788574219
    196 74.42706298828125
    197 72.20417785644531
    198 70.049072265625
    199 67.96214294433594
    200 65.93800354003906
    201 63.98081970214844
    202 62.081905364990234
    203 60.24091720581055
    204 58.457027435302734
    205 56.72966384887695
    206 55.05583190917969
    207 53.43071746826172
    208 51.85721969604492
    209 50.330406188964844
    210 48.853294372558594
    211 47.41990661621094
    212 46.02849197387695
    213 44.68041229248047
    214 43.372406005859375
    215 42.10704040527344
    216 40.87778854370117
    217 39.685672760009766
    218 38.52944564819336
    219 37.40904235839844
    220 36.32259750366211
    221 35.26750183105469
    222 34.24412536621094
    223 33.25189971923828
    224 32.290382385253906
    225 31.35875129699707
    226 30.456039428710938
    227 29.57623291015625
    228 28.724651336669922
    229 27.897735595703125
    230 27.09467887878418
    231 26.315433502197266
    232 25.559024810791016
    233 24.826108932495117
    234 24.11490821838379
    235 23.42433738708496
    236 22.75417709350586
    237 22.1038875579834
    238 21.47378921508789
    239 20.860986709594727
    240 20.26665496826172
    241 19.68951416015625
    242 19.129297256469727
    243 18.585830688476562
    244 18.057662963867188
    245 17.545291900634766
    246 17.0478572845459
    247 16.564882278442383
    248 16.096208572387695
    249 15.642477035522461
    250 15.201027870178223
    251 14.771498680114746
    252 14.354628562927246
    253 13.949911117553711
    254 13.556266784667969
    255 13.174358367919922
    256 12.803556442260742
    257 12.443826675415039
    258 12.094182968139648
    259 11.754055976867676
    260 11.424214363098145
    261 11.103830337524414
    262 10.792688369750977
    263 10.490301132202148
    264 10.196457862854004
    265 9.910770416259766
    266 9.634020805358887
    267 9.365073204040527
    268 9.103498458862305
    269 8.849289894104004
    270 8.602250099182129
    271 8.362701416015625
    272 8.129758834838867
    273 7.904178619384766
    274 7.684447288513184
    275 7.4708571434021
    276 7.263154983520508
    277 7.061304569244385
    278 6.865261077880859
    279 6.674744606018066
    280 6.4893293380737305
    281 6.309834003448486
    282 6.1348724365234375
    283 5.964991569519043
    284 5.799680233001709
    285 5.63920783996582
    286 5.483482837677002
    287 5.331809043884277
    288 5.18461275100708
    289 5.04152250289917
    290 4.90210485458374
    291 4.767287731170654
    292 4.6357574462890625
    293 4.507718086242676
    294 4.38368558883667
    295 4.2628173828125
    296 4.145517349243164
    297 4.031484603881836
    298 3.9210429191589355
    299 3.8135323524475098
    300 3.7087669372558594
    301 3.606804370880127
    302 3.507784366607666
    303 3.4115216732025146
    304 3.3178868293762207
    305 3.227001428604126
    306 3.1387429237365723
    307 3.0527186393737793
    308 2.9689369201660156
    309 2.8877618312835693
    310 2.80861759185791
    311 2.7319653034210205
    312 2.657128095626831
    313 2.5845539569854736
    314 2.5139267444610596
    315 2.4452381134033203
    316 2.3784899711608887
    317 2.313446044921875
    318 2.250396728515625
    319 2.1890132427215576
    320 2.1293528079986572
    321 2.0712552070617676
    322 2.01491379737854
    323 1.9599628448486328
    324 1.9068379402160645
    325 1.8549458980560303
    326 1.8044012784957886
    327 1.7554709911346436
    328 1.7076568603515625
    329 1.6612787246704102
    330 1.616133451461792
    331 1.5721832513809204
    332 1.5295500755310059
    333 1.488060712814331
    334 1.4476560354232788
    335 1.4082658290863037
    336 1.3700791597366333
    337 1.3330020904541016
    338 1.2968183755874634
    339 1.261742115020752
    340 1.2273691892623901
    341 1.1941691637039185
    342 1.161803960800171
    343 1.1304666996002197
    344 1.0997872352600098
    345 1.0700562000274658
    346 1.0410369634628296
    347 1.012978434562683
    348 0.9856001734733582
    349 0.9588797688484192
    350 0.9330411553382874
    351 0.9078928828239441
    352 0.8833928108215332
    353 0.8595109581947327
    354 0.8363422155380249
    355 0.8137132525444031
    356 0.7918300032615662
    357 0.7704979181289673
    358 0.7496511936187744
    359 0.7293777465820312
    360 0.7097415328025818
    361 0.6907302141189575
    362 0.6720819473266602
    363 0.6539756655693054
    364 0.6363227963447571
    365 0.6191878914833069
    366 0.6024981737136841
    367 0.5862923264503479
    368 0.5704933404922485
    369 0.5550962686538696
    370 0.5402443408966064
    371 0.5257006883621216
    372 0.5116158127784729
    373 0.49778860807418823
    374 0.4844028949737549
    375 0.47144797444343567
    376 0.4587354362010956
    377 0.4464520215988159
    378 0.4344913959503174
    379 0.42283087968826294
    380 0.4115549325942993
    381 0.4004383385181427
    382 0.38967567682266235
    383 0.3792385458946228
    384 0.369026243686676
    385 0.35914212465286255
    386 0.3495188057422638
    387 0.34017080068588257
    388 0.33102619647979736
    389 0.3221626281738281
    390 0.31351926922798157
    391 0.3051045536994934
    392 0.29698389768600464
    393 0.2890431582927704
    394 0.2812862992286682
    395 0.27377548813819885
    396 0.2664315402507782
    397 0.2593071460723877
    398 0.2523666322231293
    399 0.24562767148017883
    400 0.23905885219573975
    401 0.2326696515083313
    402 0.22642943263053894
    403 0.22041596472263336
    404 0.21451526880264282
    405 0.208772212266922
    406 0.2032390832901001
    407 0.19780001044273376
    408 0.19249044358730316
    409 0.18736113607883453
    410 0.18237529695034027
    411 0.1775200515985489
    412 0.17278337478637695
    413 0.16815100610256195
    414 0.16365650296211243
    415 0.1593281626701355
    416 0.15505871176719666
    417 0.15094691514968872
    418 0.1469399780035019
    419 0.1429978758096695
    420 0.13921505212783813
    421 0.13547922670841217
    422 0.1318729966878891
    423 0.12834791839122772
    424 0.1249440461397171
    425 0.12164192646741867
    426 0.11839407682418823
    427 0.11524495482444763
    428 0.11217689514160156
    429 0.10916300117969513
    430 0.10628639161586761
    431 0.10344935208559036
    432 0.10070722550153732
    433 0.09802745282649994
    434 0.09543640911579132
    435 0.09291282296180725
    436 0.09046123176813126
    437 0.08805195242166519
    438 0.08572274446487427
    439 0.0834435448050499
    440 0.08122450858354568
    441 0.07907608151435852
    442 0.076989084482193
    443 0.07493942230939865
    444 0.07295087724924088
    445 0.0710083395242691
    446 0.06912941485643387
    447 0.0673111230134964
    448 0.06553210318088531
    449 0.06378669291734695
    450 0.06210041046142578
    451 0.06045606732368469
    452 0.05884586274623871
    453 0.0572967603802681
    454 0.05578964576125145
    455 0.05431167781352997
    456 0.05287942290306091
    457 0.051486168056726456
    458 0.050136446952819824
    459 0.04880190268158913
    460 0.04752970486879349
    461 0.0462583526968956
    462 0.04504167288541794
    463 0.04386363923549652
    464 0.042707983404397964
    465 0.041594404727220535
    466 0.04049166291952133
    467 0.03943157196044922
    468 0.038393884897232056
    469 0.03738272935152054
    470 0.036400388926267624
    471 0.0354452058672905
    472 0.03451777622103691
    473 0.03361675143241882
    474 0.03272942081093788
    475 0.0318852998316288
    476 0.031052274629473686
    477 0.030230706557631493
    478 0.02944009006023407
    479 0.028667649254202843
    480 0.027914471924304962
    481 0.027184929698705673
    482 0.026478145271539688
    483 0.025778666138648987
    484 0.025102907791733742
    485 0.024447832256555557
    486 0.023806564509868622
    487 0.023198634386062622
    488 0.02258567325770855
    489 0.021998878568410873
    490 0.021417178213596344
    491 0.020862746983766556
    492 0.02031654305756092
    493 0.01978912204504013
    494 0.01927114464342594
    495 0.018777385354042053
    496 0.018286656588315964
    497 0.017815563827753067
    498 0.017360638827085495
    499 0.016912931576371193
    

## 使用其他梯度下降优化函数 ，Adam



```python
import torch
import torch.nn as nn

N,D_in,H,D_out=64,1000,100,10

# 随机创建一些训练数据

x=torch.randn(N,D_in)
y=torch.randn(N,D_out)

model = torch.nn.Sequential(
   torch.nn.Linear(D_in,H,bias=False),
   torch.nn.ReLU(),
   torch.nn.Linear(H,D_out,bias=False),

)


loss_fn =nn.MSELoss(reduction='sum')

learn_rate= 1e-4

optimizer = torch.optim.Adam(model.parameters(),lr=learn_rate)

for t in range(500):
        
    y_pred =model(x)
    # loss
    loss=loss_fn(y_pred,y)
    
    print(t,loss.item())                    #item拿下来，因为只能算一个
    
    optimizer.zero_grad()
    # backward pass
    # compute the gradient
    
    
    loss.backward()                          #计算图
    
    
    
    #update weights of w1 and w2
    optimizer.step()
```

    0 657.5244140625
    1 640.3297729492188
    2 623.6614990234375
    3 607.52978515625
    4 591.85009765625
    5 576.6954956054688
    6 562.1130981445312
    7 548.0004272460938
    8 534.3383178710938
    9 521.1902465820312
    10 508.44085693359375
    11 496.0226135253906
    12 483.94451904296875
    13 472.2065734863281
    14 460.7308654785156
    15 449.605224609375
    16 438.8964538574219
    17 428.43804931640625
    18 418.228271484375
    19 408.302734375
    20 398.69482421875
    21 389.3704833984375
    22 380.29608154296875
    23 371.4316101074219
    24 362.837890625
    25 354.4451904296875
    26 346.24078369140625
    27 338.247314453125
    28 330.44091796875
    29 322.8173828125
    30 315.3875427246094
    31 308.1433410644531
    32 301.0727233886719
    33 294.1838684082031
    34 287.4365539550781
    35 280.84033203125
    36 274.42120361328125
    37 268.1443176269531
    38 262.0113220214844
    39 256.0306701660156
    40 250.1688232421875
    41 244.4482421875
    42 238.8389129638672
    43 233.35800170898438
    44 227.99261474609375
    45 222.72328186035156
    46 217.5474090576172
    47 212.4734649658203
    48 207.49563598632812
    49 202.61846923828125
    50 197.8375244140625
    51 193.15875244140625
    52 188.57638549804688
    53 184.07235717773438
    54 179.6515350341797
    55 175.31460571289062
    56 171.0667724609375
    57 166.91270446777344
    58 162.83863830566406
    59 158.85470581054688
    60 154.94822692871094
    61 151.11485290527344
    62 147.36741638183594
    63 143.69342041015625
    64 140.09811401367188
    65 136.56776428222656
    66 133.09584045410156
    67 129.69561767578125
    68 126.3705825805664
    69 123.11494445800781
    70 119.92864990234375
    71 116.79975891113281
    72 113.73509216308594
    73 110.73131561279297
    74 107.78608703613281
    75 104.89933776855469
    76 102.06605529785156
    77 99.29380798339844
    78 96.58566284179688
    79 93.93988800048828
    80 91.34959411621094
    81 88.81554412841797
    82 86.34144592285156
    83 83.91876983642578
    84 81.55216217041016
    85 79.24066925048828
    86 76.98140716552734
    87 74.77218627929688
    88 72.61612701416016
    89 70.51200103759766
    90 68.45376586914062
    91 66.4464111328125
    92 64.48965454101562
    93 62.578582763671875
    94 60.716373443603516
    95 58.897857666015625
    96 57.12415313720703
    97 55.39398956298828
    98 53.70620346069336
    99 52.061058044433594
    100 50.45652389526367
    101 48.88994598388672
    102 47.36817932128906
    103 45.8847541809082
    104 44.43980407714844
    105 43.031517028808594
    106 41.661094665527344
    107 40.32606506347656
    108 39.027618408203125
    109 37.76408004760742
    110 36.533634185791016
    111 35.336307525634766
    112 34.1713981628418
    113 33.03962707519531
    114 31.941932678222656
    115 30.875654220581055
    116 29.839954376220703
    117 28.832138061523438
    118 27.85544204711914
    119 26.90619659423828
    120 25.986162185668945
    121 25.09183692932129
    122 24.22433853149414
    123 23.382797241210938
    124 22.567001342773438
    125 21.77505111694336
    126 21.00789451599121
    127 20.263912200927734
    128 19.54252052307129
    129 18.84421157836914
    130 18.167816162109375
    131 17.51216697692871
    132 16.877212524414062
    133 16.262231826782227
    134 15.668057441711426
    135 15.090913772583008
    136 14.533159255981445
    137 13.992703437805176
    138 13.470781326293945
    139 12.965850830078125
    140 12.476560592651367
    141 12.004070281982422
    142 11.547585487365723
    143 11.106719017028809
    144 10.680509567260742
    145 10.268623352050781
    146 9.870786666870117
    147 9.486197471618652
    148 9.115273475646973
    149 8.757125854492188
    150 8.410985946655273
    151 8.07724666595459
    152 7.75526237487793
    153 7.44491720199585
    154 7.145737648010254
    155 6.857034206390381
    156 6.5795135498046875
    157 6.311898231506348
    158 6.054271221160889
    159 5.806034088134766
    160 5.566842555999756
    161 5.336662769317627
    162 5.114632606506348
    163 4.900777816772461
    164 4.695301055908203
    165 4.497420310974121
    166 4.307158946990967
    167 4.124291896820068
    168 3.9482147693634033
    169 3.7791175842285156
    170 3.6166532039642334
    171 3.4604098796844482
    172 3.311136484146118
    173 3.167300224304199
    174 3.029513359069824
    175 2.8971054553985596
    176 2.770145893096924
    177 2.648594379425049
    178 2.5320725440979004
    179 2.4203786849975586
    180 2.312748908996582
    181 2.209258794784546
    182 2.1097304821014404
    183 2.0139458179473877
    184 1.922104001045227
    185 1.8339300155639648
    186 1.749298334121704
    187 1.668078899383545
    188 1.590322732925415
    189 1.5158507823944092
    190 1.4445093870162964
    191 1.3764790296554565
    192 1.3112000226974487
    193 1.24888277053833
    194 1.1893107891082764
    195 1.1323353052139282
    196 1.0779954195022583
    197 1.0260705947875977
    198 0.9764898419380188
    199 0.9291024208068848
    200 0.8839092254638672
    201 0.8407554626464844
    202 0.7995792627334595
    203 0.7603522539138794
    204 0.7229159474372864
    205 0.6872016191482544
    206 0.6531648635864258
    207 0.6207209825515747
    208 0.5898053050041199
    209 0.5603075623512268
    210 0.5322450995445251
    211 0.5055277347564697
    212 0.4800887703895569
    213 0.45581620931625366
    214 0.43272143602371216
    215 0.41088467836380005
    216 0.3901159167289734
    217 0.3703466057777405
    218 0.3515405058860779
    219 0.33364808559417725
    220 0.3166348934173584
    221 0.30045199394226074
    222 0.2850603461265564
    223 0.2704242467880249
    224 0.2565053701400757
    225 0.24328432977199554
    226 0.23072123527526855
    227 0.21878235042095184
    228 0.20742419362068176
    229 0.19663693010807037
    230 0.1863933950662613
    231 0.17666232585906982
    232 0.16743206977844238
    233 0.1586887538433075
    234 0.1503916233778
    235 0.14251792430877686
    236 0.13504405319690704
    237 0.127948597073555
    238 0.12122249603271484
    239 0.11482558399438858
    240 0.10878297686576843
    241 0.10308966785669327
    242 0.09768633544445038
    243 0.09255825728178024
    244 0.08769857883453369
    245 0.08308611810207367
    246 0.07871423661708832
    247 0.07456980645656586
    248 0.0706365779042244
    249 0.06690753251314163
    250 0.06337437033653259
    251 0.06002150475978851
    252 0.056844230741262436
    253 0.053835101425647736
    254 0.0509781576693058
    255 0.048272568732500076
    256 0.0457131452858448
    257 0.043288834393024445
    258 0.04099002853035927
    259 0.038811445236206055
    260 0.03674566373229027
    261 0.034787047654390335
    262 0.0329345241189003
    263 0.03117307275533676
    264 0.029505755752325058
    265 0.027927979826927185
    266 0.026431823149323463
    267 0.02501433528959751
    268 0.02367122657597065
    269 0.022399544715881348
    270 0.021194718778133392
    271 0.020053785294294357
    272 0.018973425030708313
    273 0.017949916422367096
    274 0.016980281099677086
    275 0.01606343872845173
    276 0.015193522907793522
    277 0.014370597898960114
    278 0.013591560535132885
    279 0.012854205444455147
    280 0.01215558685362339
    281 0.011494896374642849
    282 0.010869273915886879
    283 0.01027723215520382
    284 0.009717085398733616
    285 0.00918669905513525
    286 0.00868508592247963
    287 0.00820997916162014
    288 0.007760634180158377
    289 0.007335263770073652
    290 0.00693298876285553
    291 0.006552496459335089
    292 0.006192388478666544
    293 0.005851631984114647
    294 0.005529429763555527
    295 0.00522475503385067
    296 0.0049362932331860065
    297 0.004663657862693071
    298 0.0044058505445718765
    299 0.0041619702242314816
    300 0.003931453451514244
    301 0.0037135726306587458
    302 0.0035073342733085155
    303 0.003312431974336505
    304 0.0031282096169888973
    305 0.0029540357645601034
    306 0.0027894400991499424
    307 0.0026338521856814623
    308 0.0024867465253919363
    309 0.002347822766751051
    310 0.0022164522670209408
    311 0.002092381939291954
    312 0.0019751456566154957
    313 0.0018643003422766924
    314 0.0017596251564100385
    315 0.0016607233555987477
    316 0.0015673527959734201
    317 0.0014791369903832674
    318 0.0013958202907815576
    319 0.0013170548481866717
    320 0.001242763944901526
    321 0.0011725359363481402
    322 0.0011063010897487402
    323 0.001043680589646101
    324 0.0009846051689237356
    325 0.0009289211593568325
    326 0.0008761862409301102
    327 0.0008264863863587379
    328 0.0007795765995979309
    329 0.0007353017572313547
    330 0.0006934852572157979
    331 0.0006540297181345522
    332 0.0006168254767544568
    333 0.000581693893764168
    334 0.0005485501023940742
    335 0.0005172791425138712
    336 0.00048776750918477774
    337 0.0004599065287038684
    338 0.0004336455021984875
    339 0.00040886813076213
    340 0.000385495979571715
    341 0.00036348728463053703
    342 0.0003426464681979269
    343 0.00032303069019690156
    344 0.0003045421908609569
    345 0.0002870996540877968
    346 0.0002706417581066489
    347 0.0002551348879933357
    348 0.00024050151114352047
    349 0.0002267066593049094
    350 0.000213699386222288
    351 0.0002014342462643981
    352 0.00018987638759426773
    353 0.00017897345242090523
    354 0.0001686971663730219
    355 0.00015901285223662853
    356 0.00014988075417932123
    357 0.00014127205940894783
    358 0.00013316671538632363
    359 0.00012551828694995493
    360 0.00011831063602585346
    361 0.00011151382932439446
    362 0.00010511427535675466
    363 9.9073069577571e-05
    364 9.339053212897852e-05
    365 8.80265433806926e-05
    366 8.297806925838813e-05
    367 7.821650797268376e-05
    368 7.372590334853157e-05
    369 6.950337410671636e-05
    370 6.55146359349601e-05
    371 6.176109309308231e-05
    372 5.822513412567787e-05
    373 5.488604074344039e-05
    374 5.174710531719029e-05
    375 4.878640902461484e-05
    376 4.599364547175355e-05
    377 4.336627171142027e-05
    378 4.088879359187558e-05
    379 3.855166141875088e-05
    380 3.6352725146571174e-05
    381 3.427847332204692e-05
    382 3.232403832953423e-05
    383 3.0481414796668105e-05
    384 2.8746713724103756e-05
    385 2.710863554966636e-05
    386 2.5569745048414916e-05
    387 2.4113225663313642e-05
    388 2.2746826289221644e-05
    389 2.145283178833779e-05
    390 2.02384362637531e-05
    391 1.909131242427975e-05
    392 1.8009992345469072e-05
    393 1.699020322121214e-05
    394 1.6029316611820832e-05
    395 1.5125167919904925e-05
    396 1.4270723113440908e-05
    397 1.3466038581100293e-05
    398 1.2705679182545282e-05
    399 1.1991054634563625e-05
    400 1.1316436939523555e-05
    401 1.0679501428967342e-05
    402 1.008031267701881e-05
    403 9.513659279036801e-06
    404 8.981101927929558e-06
    405 8.476999028061982e-06
    406 8.004213668755256e-06
    407 7.556029686384136e-06
    408 7.134205588954501e-06
    409 6.73630938763381e-06
    410 6.360614406730747e-06
    411 6.005427167110611e-06
    412 5.671589860867243e-06
    413 5.356075234885793e-06
    414 5.059116119809914e-06
    415 4.778173206432257e-06
    416 4.512859959504567e-06
    417 4.2620772546797525e-06
    418 4.02625937567791e-06
    419 3.8039011087676045e-06
    420 3.5932355331169674e-06
    421 3.3953106139961164e-06
    422 3.2075183753477177e-06
    423 3.0300984690256882e-06
    424 2.8632475732592866e-06
    425 2.706038912947406e-06
    426 2.5573106086085318e-06
    427 2.4164655769709498e-06
    428 2.283508365508169e-06
    429 2.1578998712357134e-06
    430 2.0398338165250607e-06
    431 1.9278847958048573e-06
    432 1.8222491462438484e-06
    433 1.7228281876668916e-06
    434 1.628028371669643e-06
    435 1.5390631915579434e-06
    436 1.4547982800650061e-06
    437 1.3755671943727066e-06
    438 1.3002099876757711e-06
    439 1.2291832263144897e-06
    440 1.1618517419265117e-06
    441 1.0984072105202358e-06
    442 1.0386861504230183e-06
    443 9.818314765652758e-07
    444 9.284475481763366e-07
    445 8.779786071499984e-07
    446 8.298916327476036e-07
    447 7.844554374969448e-07
    448 7.418659606628353e-07
    449 7.013462095528666e-07
    450 6.634513738390524e-07
    451 6.270612971093215e-07
    452 5.929975372964691e-07
    453 5.607414550468093e-07
    454 5.301439500726701e-07
    455 5.012357746636553e-07
    456 4.740399504044035e-07
    457 4.4810559529651073e-07
    458 4.237831774389633e-07
    459 4.00664845301435e-07
    460 3.7888375459260715e-07
    461 3.5803364539788163e-07
    462 3.385246145626297e-07
    463 3.2001992167352e-07
    464 3.0257041316872346e-07
    465 2.8607882995856926e-07
    466 2.7055412488152797e-07
    467 2.557438847361482e-07
    468 2.41612724494189e-07
    469 2.285273410507216e-07
    470 2.159925998057588e-07
    471 2.041156790255627e-07
    472 1.9305795717627916e-07
    473 1.822372865944999e-07
    474 1.7232144955414697e-07
    475 1.6285397919091338e-07
    476 1.5408963349727856e-07
    477 1.4551946492247225e-07
    478 1.3748193339324644e-07
    479 1.2993409370665177e-07
    480 1.2284041872590024e-07
    481 1.1611712125159102e-07
    482 1.096491715202319e-07
    483 1.0353434731769084e-07
    484 9.794163702281367e-08
    485 9.254716815121355e-08
    486 8.74303935916032e-08
    487 8.261812212140285e-08
    488 7.809104118905452e-08
    489 7.379856015177211e-08
    490 6.96168171998579e-08
    491 6.57685177429812e-08
    492 6.215736902959179e-08
    493 5.8730783791816066e-08
    494 5.5440548152319025e-08
    495 5.2362111091497354e-08
    496 4.9446157390775625e-08
    497 4.670716435839495e-08
    498 4.409406173522257e-08
    499 4.1707274078817136e-08
    

### 自定义模型



```python
import torch
import torch.nn as nn

N,D_in,H,D_out=64,1000,100,10

# 随机创建一些训练数据

x=torch.randn(N,D_in)
y=torch.randn(N,D_out)

class TwoLayerNet(torch.nn.Module):
    def __init__(self, D_in, H, D_out):
        super(TwoLayerNet, self).__init__()
        self.liner1 = torch.nn.Linear(D_in, H)
        self.liner2 = torch.nn.Linear(H, D_out)
    def forward(self,x):
        y_pred = self.liner2(self.liner1(x).clamp(min=0))
        return y_pred
        
              
              


model = TwoLayerNet(D_in,H,D_out)
loss_fn =nn.MSELoss(reduction='sum')

learn_rate= 1e-4

optimizer = torch.optim.Adam(model.parameters(),lr=learn_rate)

for t in range(500):
        
    y_pred =model(x)
    # loss
    loss=loss_fn(y_pred,y)
    
    print(t,loss.item())                    #item拿下来，因为只能算一个
    
    optimizer.zero_grad()
    # backward pass
    # compute the gradient
    
    
    loss.backward()                          #计算图
    
    
    
    #update weights of w1 and w2
    optimizer.step()
```

    0 675.071533203125
    1 657.800048828125
    2 640.9560546875
    3 624.6432495117188
    4 608.81298828125
    5 593.5341796875
    6 578.6674194335938
    7 564.1860961914062
    8 550.11328125
    9 536.4168701171875
    10 523.1695556640625
    11 510.3012390136719
    12 497.77166748046875
    13 485.57366943359375
    14 473.695556640625
    15 462.1455383300781
    16 450.9773864746094
    17 440.07550048828125
    18 429.51495361328125
    19 419.2410888671875
    20 409.21661376953125
    21 399.46484375
    22 389.8871765136719
    23 380.50323486328125
    24 371.353515625
    25 362.4820251464844
    26 353.8740539550781
    27 345.4752197265625
    28 337.2479248046875
    29 329.2089538574219
    30 321.33648681640625
    31 313.6104431152344
    32 306.06683349609375
    33 298.6894836425781
    34 291.4845275878906
    35 284.4430847167969
    36 277.53631591796875
    37 270.7483825683594
    38 264.09185791015625
    39 257.5774230957031
    40 251.2073516845703
    41 244.9727783203125
    42 238.84844970703125
    43 232.8526611328125
    44 226.9870147705078
    45 221.2694549560547
    46 215.65652465820312
    47 210.15174865722656
    48 204.7541961669922
    49 199.4863739013672
    50 194.33905029296875
    51 189.2979736328125
    52 184.35443115234375
    53 179.50296020507812
    54 174.76719665527344
    55 170.123046875
    56 165.5731201171875
    57 161.11868286132812
    58 156.76654052734375
    59 152.4940643310547
    60 148.30953979492188
    61 144.21926879882812
    62 140.2135009765625
    63 136.30345153808594
    64 132.4679718017578
    65 128.71607971191406
    66 125.03540802001953
    67 121.42881774902344
    68 117.90042877197266
    69 114.4529037475586
    70 111.08943176269531
    71 107.80319213867188
    72 104.59178161621094
    73 101.45782470703125
    74 98.40000915527344
    75 95.41193389892578
    76 92.49285888671875
    77 89.64396667480469
    78 86.86814880371094
    79 84.16051483154297
    80 81.51704406738281
    81 78.93827819824219
    82 76.42691040039062
    83 73.9809341430664
    84 71.59559631347656
    85 69.26870727539062
    86 67.00127410888672
    87 64.79551696777344
    88 62.64559555053711
    89 60.55546188354492
    90 58.52265167236328
    91 56.54796600341797
    92 54.633304595947266
    93 52.77513885498047
    94 50.969120025634766
    95 49.213409423828125
    96 47.508914947509766
    97 45.853904724121094
    98 44.24819564819336
    99 42.6901741027832
    100 41.1772575378418
    101 39.71067428588867
    102 38.29155349731445
    103 36.91522216796875
    104 35.58151626586914
    105 34.290653228759766
    106 33.039730072021484
    107 31.830429077148438
    108 30.661630630493164
    109 29.5322208404541
    110 28.44089126586914
    111 27.383420944213867
    112 26.362686157226562
    113 25.376296997070312
    114 24.42588233947754
    115 23.505828857421875
    116 22.615798950195312
    117 21.75764274597168
    118 20.92774772644043
    119 20.126089096069336
    120 19.35154151916504
    121 18.603717803955078
    122 17.880126953125
    123 17.183361053466797
    124 16.51020050048828
    125 15.861223220825195
    126 15.233556747436523
    127 14.628260612487793
    128 14.044479370117188
    129 13.481939315795898
    130 12.93985652923584
    131 12.416297912597656
    132 11.911966323852539
    133 11.425786972045898
    134 10.957484245300293
    135 10.506603240966797
    136 10.071609497070312
    137 9.653145790100098
    138 9.250304222106934
    139 8.863102912902832
    140 8.490753173828125
    141 8.132308959960938
    142 7.787549018859863
    143 7.45601749420166
    144 7.137315273284912
    145 6.830944061279297
    146 6.536983013153076
    147 6.254375457763672
    148 5.982782363891602
    149 5.722103595733643
    150 5.471868991851807
    151 5.231784343719482
    152 5.001473903656006
    153 4.780768394470215
    154 4.56916618347168
    155 4.366157054901123
    156 4.171532154083252
    157 3.9848947525024414
    158 3.8059749603271484
    159 3.634495258331299
    160 3.4702250957489014
    161 3.312696933746338
    162 3.161724328994751
    163 3.0172760486602783
    164 2.8788022994995117
    165 2.746354103088379
    166 2.6195240020751953
    167 2.498171806335449
    168 2.381962299346924
    169 2.270756721496582
    170 2.164482593536377
    171 2.062793731689453
    172 1.9656682014465332
    173 1.8725988864898682
    174 1.7837796211242676
    175 1.698994517326355
    176 1.618093490600586
    177 1.5408155918121338
    178 1.4671752452850342
    179 1.3967851400375366
    180 1.3297364711761475
    181 1.2657248973846436
    182 1.2047806978225708
    183 1.1467328071594238
    184 1.0913814306259155
    185 1.0386351346969604
    186 0.9883777499198914
    187 0.9404821991920471
    188 0.8947550654411316
    189 0.8512371182441711
    190 0.8097254633903503
    191 0.7702057957649231
    192 0.7325569987297058
    193 0.6967023015022278
    194 0.6625503301620483
    195 0.630010724067688
    196 0.5990220904350281
    197 0.569519579410553
    198 0.5414331555366516
    199 0.5147745013237
    200 0.4893239736557007
    201 0.46514126658439636
    202 0.4421236515045166
    203 0.42019519209861755
    204 0.39936554431915283
    205 0.37951138615608215
    206 0.36062708497047424
    207 0.34266769886016846
    208 0.3255847990512848
    209 0.30931755900382996
    210 0.29385611414909363
    211 0.2791409492492676
    212 0.2651471495628357
    213 0.25184381008148193
    214 0.23920592665672302
    215 0.22714370489120483
    216 0.21567821502685547
    217 0.20477379858493805
    218 0.1944233477115631
    219 0.18459872901439667
    220 0.175257608294487
    221 0.16636669635772705
    222 0.15793150663375854
    223 0.14990805089473724
    224 0.1422790139913559
    225 0.13503214716911316
    226 0.12815381586551666
    227 0.12161066383123398
    228 0.11540178954601288
    229 0.10950028151273727
    230 0.10389295220375061
    231 0.09857141971588135
    232 0.09352198243141174
    233 0.08871525526046753
    234 0.0841558650135994
    235 0.07982668280601501
    236 0.07571610808372498
    237 0.07181394845247269
    238 0.06810890883207321
    239 0.06459061801433563
    240 0.06124960631132126
    241 0.05807875841856003
    242 0.05507994443178177
    243 0.052238211035728455
    244 0.049534108489751816
    245 0.04696948081254959
    246 0.04453727975487709
    247 0.042229145765304565
    248 0.0400395467877388
    249 0.03796123340725899
    250 0.03599665313959122
    251 0.03414104878902435
    252 0.03238093852996826
    253 0.030709223821759224
    254 0.029122859239578247
    255 0.02761700563132763
    256 0.02618797868490219
    257 0.02483171969652176
    258 0.023543942719697952
    259 0.022322261705994606
    260 0.021162839606404305
    261 0.02006220817565918
    262 0.019018394872546196
    263 0.018028251826763153
    264 0.01708962582051754
    265 0.016196927055716515
    266 0.015351744368672371
    267 0.014549719169735909
    268 0.013788745738565922
    269 0.01306674163788557
    270 0.01238139346241951
    271 0.011731528677046299
    272 0.01111479289829731
    273 0.010529882274568081
    274 0.009974762797355652
    275 0.009448515251278877
    276 0.008949032984673977
    277 0.00847649946808815
    278 0.008027256466448307
    279 0.007601574063301086
    280 0.007197420112788677
    281 0.006814644206315279
    282 0.006451902911067009
    283 0.006107539404183626
    284 0.005781307816505432
    285 0.005471949465572834
    286 0.005178690887987614
    287 0.004900788422673941
    288 0.004637314006686211
    289 0.0043876594863832
    290 0.004150853492319584
    291 0.0039277817122638226
    292 0.003717106766998768
    293 0.0035172095522284508
    294 0.0033277119509875774
    295 0.0031483680941164494
    296 0.002978427102789283
    297 0.0028174826875329018
    298 0.002664918079972267
    299 0.002520364709198475
    300 0.002383420942351222
    301 0.002253732644021511
    302 0.002130840439349413
    303 0.00201443862169981
    304 0.0019042586209252477
    305 0.0017998521216213703
    306 0.0017011354211717844
    307 0.001607531332410872
    308 0.001518834731541574
    309 0.0014349883422255516
    310 0.0013555956538766623
    311 0.0012804324505850673
    312 0.0012092965189367533
    313 0.0011419832007959485
    314 0.0010782857425510883
    315 0.0010179742239415646
    316 0.0009609485859982669
    317 0.0009069961379282176
    318 0.0008559569832868874
    319 0.0008077537058852613
    320 0.0007621180266141891
    321 0.0007189794559963048
    322 0.000678161857649684
    323 0.0006396070821210742
    324 0.0006031760131008923
    325 0.0005687445518560708
    326 0.0005362107767723501
    327 0.0005054729990661144
    328 0.00047641689889132977
    329 0.0004489723942242563
    330 0.00042306073009967804
    331 0.00039858976379036903
    332 0.0003754838544409722
    333 0.00035369498073123395
    334 0.00033310012076981366
    335 0.00031363379093818367
    336 0.0002952930226456374
    337 0.0002779841306619346
    338 0.0002616533311083913
    339 0.000246247072936967
    340 0.00023171967768575996
    341 0.000218011366087012
    342 0.00020508405577857047
    343 0.00019289417832624167
    344 0.00018140320025850087
    345 0.0001705938921077177
    346 0.000160379393491894
    347 0.00015075404371600598
    348 0.0001416982850059867
    349 0.00013316361582838
    350 0.00012513136607594788
    351 0.00011755872401408851
    352 0.00011042744154110551
    353 0.00010371221287641674
    354 9.73916903603822e-05
    355 9.144121577264741e-05
    356 8.584572788095102e-05
    357 8.058379899011925e-05
    358 7.56283916416578e-05
    359 7.096175977494568e-05
    360 6.658074562437832e-05
    361 6.245922850212082e-05
    362 5.858340591657907e-05
    363 5.493903154274449e-05
    364 5.1514620281523094e-05
    365 4.829352110391483e-05
    366 4.5267595851328224e-05
    367 4.24266436311882e-05
    368 3.9757902413839474e-05
    369 3.7248020817060024e-05
    370 3.489652226562612e-05
    371 3.2687730708858e-05
    372 3.0611816328018904e-05
    373 2.8662827389780432e-05
    374 2.6832320145331323e-05
    375 2.5117480618064292e-05
    376 2.3509190214099362e-05
    377 2.199754999310244e-05
    378 2.057825076917652e-05
    379 1.9250635887146927e-05
    380 1.8006521713687107e-05
    381 1.684006019786466e-05
    382 1.5743238691356964e-05
    383 1.471703581046313e-05
    384 1.3757194210484158e-05
    385 1.2856111425207928e-05
    386 1.2012415027129464e-05
    387 1.1224025001865812e-05
    388 1.048484682542039e-05
    389 9.791344382392708e-06
    390 9.141098416876048e-06
    391 8.5353867689264e-06
    392 7.967241799633484e-06
    393 7.436241958203027e-06
    394 6.937753823876847e-06
    395 6.4739783738332335e-06
    396 6.0372622101567686e-06
    397 5.6314434004889335e-06
    398 5.250363756204024e-06
    399 4.895597157883458e-06
    400 4.563316451822175e-06
    401 4.253771749063162e-06
    402 3.964468305639457e-06
    403 3.6935057323717047e-06
    404 3.440500677243108e-06
    405 3.2035550248110667e-06
    406 2.9835086934326682e-06
    407 2.777466534098494e-06
    408 2.5854494651866844e-06
    409 2.4066139303613454e-06
    410 2.2388221623259597e-06
    411 2.0832594600506127e-06
    412 1.937739170898567e-06
    413 1.8021599998974125e-06
    414 1.6758106085035251e-06
    415 1.5577672911604168e-06
    416 1.4480735899269348e-06
    417 1.345363102700503e-06
    418 1.2510382703112555e-06
    419 1.16132241601008e-06
    420 1.0787299515868654e-06
    421 1.001697114588751e-06
    422 9.301364798375289e-07
    423 8.631295713712461e-07
    424 8.014047807591851e-07
    425 7.437906219820434e-07
    426 6.901153710714425e-07
    427 6.40138694052439e-07
    428 5.932387239226955e-07
    429 5.500281758941128e-07
    430 5.100524163026421e-07
    431 4.729684803805867e-07
    432 4.3817865957862523e-07
    433 4.058859417455096e-07
    434 3.7583629364235094e-07
    435 3.483625619082886e-07
    436 3.226822400392848e-07
    437 2.9859970140933e-07
    438 2.761523774097441e-07
    439 2.557888763021765e-07
    440 2.3681434413447278e-07
    441 2.1871056787858834e-07
    442 2.0236518594174413e-07
    443 1.8739733320671803e-07
    444 1.7312987665718538e-07
    445 1.6004103997602215e-07
    446 1.4778291301809077e-07
    447 1.3658269892857788e-07
    448 1.2626061618448148e-07
    449 1.1642626418506552e-07
    450 1.0778631320818022e-07
    451 9.929058109037214e-08
    452 9.175942494721312e-08
    453 8.46875849447315e-08
    454 7.81139704031375e-08
    455 7.19282979844138e-08
    456 6.650468264979281e-08
    457 6.125350182628608e-08
    458 5.64460442831205e-08
    459 5.2010065587637655e-08
    460 4.786760499086995e-08
    461 4.414367182903334e-08
    462 4.069916670346174e-08
    463 3.74234438993426e-08
    464 3.4501645984619245e-08
    465 3.176009144567615e-08
    466 2.9306942650464407e-08
    467 2.693903944361864e-08
    468 2.4769455819750874e-08
    469 2.276122934574687e-08
    470 2.103338658798748e-08
    471 1.9330853362475864e-08
    472 1.779015157410413e-08
    473 1.6374210431990832e-08
    474 1.5095128702569127e-08
    475 1.3843276747138589e-08
    476 1.2769182156091574e-08
    477 1.1752807616005612e-08
    478 1.0784005688435627e-08
    479 9.939742362519155e-09
    480 9.117737675978788e-09
    481 8.384951399875717e-09
    482 7.69969243918922e-09
    483 7.111684574567789e-09
    484 6.538492858254585e-09
    485 6.021202203498888e-09
    486 5.517959422718377e-09
    487 5.072866571254053e-09
    488 4.689011845471214e-09
    489 4.325558133899676e-09
    490 3.9804270990373425e-09
    491 3.6586034202201745e-09
    492 3.3921785380641722e-09
    493 3.1208291506601427e-09
    494 2.8632245463455774e-09
    495 2.66672883775243e-09
    496 2.442687385695308e-09
    497 2.2600832316754804e-09
    498 2.104084240173165e-09
    499 1.956831585658847e-09
    


```python

```


```python

```


```python

```


```python

```
